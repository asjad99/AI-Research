
## About

As the significance of data continues to grow in society, it is essential for data-intensive computing systems to adapt accordingly. In the future:

- Data generation will continue to increase
- Data-driven organizations will become standard practice
- Organizations will understand which data to collect from the outset
- Experiment-driven optimization of operations and design will be incorporated into educational curriculums
- Machine learning will become so user-friendly that average programmers will easily utilize it to build data products
- Domain-specific startups will automate some basic data analysis tasks, addressing particular data challenges
- Data analysis will consider the context in which the data was generated
- Overcoming data challenges and extracting value from data will remain difficult
- Creative business strategies will still rely on human input



### Case Studies: 

Data Products: 

1. Fast Forwards Labs
2. Advanced Analytics in Spark 
3. Beautiful Data

With the growing importance of data in society and the increasing volume of information, it is crucial for data-intensive computing systems to advance. 

This repo inspired from work being done at stanford's  [Future Data Systems group](http://www.futuredata.io.s3-website-us-west-2.amazonaws.com/) where their mission is to design and build post-database systems that enable next-generation data-intensive applications. This group is part of [stanford Dawn]() whose work inolves democratize AI by making it dramatically easier to build AI-powered applications. 

AI advancement is driven by three factors: algorithmic innovation, data (supervised or interactive environments), and available compute power for training. Compute power is the most quantifiable of these factors, enabling us to measure AI progress. The amount of compute used to train a single model correlates with the power of our best models. The trend shows an increase by roughly a factor of 10 each year, partly driven by custom hardware like GPUs and TPUs, and mainly propelled by researchers finding ways to use more chips in parallel.

Four distinct eras are visible in AI development: before 2012, when using GPUs for ML was uncommon; 2012-2014, when infrastructure to train on multiple GPUs was rare; 2014-2016, when large-scale results used 10-100 GPUs; and 2016-2017, when greater algorithmic parallelism and specialized hardware significantly increased limits for some applications.

The trend may continue as hardware startups develop AI-specific chips that promise increased FLOPS/Watt in the next 1-2 years. Additionally, algorithmic innovations could be combined multiplicatively for further gains. However, cost and physics will eventually limit parallelism and chip efficiency. Although it is uncertain how long the trend will persist, its potential for rapid capability increases highlights the importance of addressing safety and malicious AI use now. OpenAI aims to ensure AI progress benefits humanity, offering various research and engineering roles.


![ai-and-compute-all-2](https://user-images.githubusercontent.com/3470924/118594494-e9f14480-b7ec-11eb-8c31-90e0af92e532.png)

reference: https://openai.com/research/ai-and-compute 


### Areas: 
- [Systems Development in C++/Rust]()

- [Data Engineering and MLops](https://github.com/asjad99/data-engineering-ml-ops)


#### Few old Projects: 
- [Scalable-Topic-Modeling](https://github.com/asjad99/Scalable-Topic-Modeling) - Singular Value Decomposition for Latent Semantic Indexing using PySpark
- [GA_TSP](https://github.com/asjad99/Genetic-Algorithms) - traveling salesman problem (TSP) using Genetic-Algorithms in C++
- [Hybrid-CI](https://github.com/asjad99/Hybrid-CI-System) - PCA dimensionality reduction using Genetic algorithms
- [MLP](https://github.com/asjad99/MLP) - A barebones implementation of an MLP and Backprop algorithm in C++  


