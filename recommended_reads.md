

![](https://paper-attachments.dropbox.com/s_BE529D29BFADE8EB507215F09DC98B07979B4474F792DF5A4394CDF5F178BCD3_1607429051486_Screen+Shot+2020-03-03+at+3.10.18+pm.png)

Why the thesis based approach is the best? 


    - Because vague thoughts are lurking behind in your head. its only when you debate or write they are concrete. 
    - Because our memory is imperfect
    - because we absorb knowledge differently when we are trying to assemble piences of puzzle
    - Because we can dig deep 
    
    
    
### Reading List

NNs + most exciting recent progress has been in new forms of probabilistic machine learning (Ghahramani, 2015). For example, researchers have developed automated statistical reasoning techniques (Lloyd, Duvenaud, Grosse, Tenenbaum, & Ghahramani, 2014), automated techniques for model building and selection (Grosse, Salakhutdinov, Freeman, & Tenenbaum, 2012), and probabilistic programming languages (e.g., Gelman, Lee, & Guo, 2015; Goodman, Mansinghka, Roy, Bonawitz, & Tenenbaum, 2008; Mansinghka, Selsam, & Perov, 2014). We believe that these approaches will play important roles in future AI systems, and they are at least as compatible with the ideas from cognitive science we discuss here, but a full discussion of those connections is beyond the scope of the current article.

  - A Roadmap towards Machine Intelligence (facebook): https://arxiv.org/pdf/1511.08130.pdf
  - Building Machines That Learn and Think Like People (MIT): Building Machines That Learn and Think Like People
  - AI and NeuroScience by deepmind
  - Philosophy of Computer Science by William J. Rapaport
  - RLDM Conference notes: https://david-abel.github.io/notes/rldm_2019.pdf

interesting thesis: https://cims.nyu.edu/~brenden/papers/LakePhDThesis.pdf
 http://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf
https://www.kurzweilai.net/when-machines-learn-like-humans
            
            
### Neural networks and deep learning

- McClelland, J. L., Rumelhart, D. E., & Hinton, G. E. The Appeal of Parallel Distributed Processing. Vol I, Ch 1.
- LeCun, Y., Bengio, Y. & Hinton, G. (2015). Deep learning. Nature 521:436–44.
- McClelland, J. L., & Rogers, T. T. (2003). The parallel distributed processing approach to semantic cognition. Nature Reviews Neuroscience, 4(4), 310-322.
- Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14(2), 179-211.
- Peterson, J., Abbott, J., & Griffiths, T. (2016). Adapting Deep Network Features to Capture Psychological Representations. Presented at the 38th Annual Conference of the Cognitive Science Society.

https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap 

###  Reinforcement learning and decision making

- Gureckis, T.M. and Love, B.C. (2015) Reinforcement learning: A computational perspective. Oxford Handbook of Computational and Mathematical Psychology, Edited by Busemeyer, J.R., Townsend, J., Zheng, W., and Eidels, A., Oxford University Press, New York, NY.
- Daw, N.S. (2013) “Advanced Reinforcement Learning” Chapter in Neuroeconomics: Decision making and the brain, 2nd edition
- Niv, Y. and Schoenbaum, G. (2008) “Dialogues on prediction errors” Trends in Cognitive Science, 12(7), 265-72.
- Nathaniel D. Daw, John P. O’Doherty, Peter Dayan, Ben Seymour & Raymond J. Dolan (2006). Cortical substrates for exploratory decisions in humans. Nature, 441, 876-879.
https://github.com/aikorea/awesome-rl 

###  Bayesian modeling

- Russel, S. J., and Norvig, P. Artificial Intelligence: A Modern Approach. Chapter 13, Uncertainty.
- Tenenbaum, J. B., and Griffiths, T. L. (2001). Generalization, similarity, and Bayesian inference. Behavioral and Brain Sciences, 24(4), 629-640.
- Tenenbaum, J. B., Kemp, C., Griffiths, T. L., & Goodman, N. D. (2011). How to grow a mind: Statistics, structure, and abstraction. Science, 331(6022), 1279-1285.
- Ghahramani, Z. (2015). Probabilistic machine learning and artificial intelligence. Nature, 521(7553), 452.
- MacKay, D. (2003). Chapter 29: Monte Carlo Methods. In Information Theory, Inference, and Learning Algorithms.


###  Rational versus mechanistic modeling approaches

- Jones, M. & Love, B.C. (2011). Bayesian Fundamentalism or Enlightenment? On the Explanatory Status and Theoretical Contributions of Bayesian Models of Cognition. Behavioral and Brain Sciences (target article).
- Griffiths, T.L., Lieder, F., & Goodman, N.D. (2015). Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic. Topics in Cognitive Science, 7(2), 217-229.


###  Model comparison and fitting, tricks of trade

- Wilson, R.C. and Collins, A.G.E. (2019). Ten simple rules for the computational modeling of behavioral data. eLife 2019;8:e49547
- Pitt, M.A. and Myung, J (2002) When a good fit can be bad. Trends in Cognitive Science, 6, 10, 421-425.
- Roberts, S. & Pashler, H. (2000) How persuasive is a good fit? A comment on theory testing. Psychological Review, 107, 358-367.
- [optional] Myung, I.J. (2003). Tutorial on maximum likelihood estimation. Journal of Mathematical Psychology, 47, 90-100.


Probabilistic graphical models

- Charniak (1991). Bayesian networks without tears. AI Magazine, 50-63.
- Kemp, C., & Tenenbaum, J. B. (2008). The discovery of structural form. Proceedings of the National Academy of Sciences, 105(31), 10687-10692.
- [optional] Russel, S. J., and Norvig, P. Artificial Intelligence: A Modern Approach. Chapter 14, Probabilistic reasoning systems.


Program induction and language of thought models

- Ghahramani, Z. (2015). Probabilistic machine learning and artificial intelligence. Nature, 521(7553), 452.
- Goodman, N. D., Tenenbaum, J. B., & Gerstenberg, T. (2014). Concepts in a probabilistic language of thought. Center for Brains, Minds and Machines (CBMM).
- Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.

Computational Cognitive Neuroscience

- Kreigeskorte, N. and Douglas, P.K. (2018) Cognitive computational neuroscience. Nature Neuroscience. 21(9): 1148-1160. doi:10.1038/s41593-018-0210-5
- Turner, B.M., Forstmann, B.U., Love, B.C., Palmeri, T.J., Van Maanen, L. (2017). Approaches to analysis in model-based cognitive neuroscience. Journal of Mathematical Psychology. 76(B), 65-79.

###  Introduction and overview

- No assigned readings

###  Deep learning – Lecture

- LeCun, Y., Bengio, Y. & Hinton, G. (2015). Deep learning. Nature 521:436–44.
- Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (pp. 1097-1105).
- No reaction post is required for these readings

###  Deep learning - Discussion

- Lake, B. M., Ullman, T. D., Tenenbaum, J. B., Gerhsman, S. J. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences. Only Sections 1-3 (pgs. 1-9)
- Mnih, V., Kavukcuoglu, K., Silver, D., …. & Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature 518(7540):529–33.
- Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., … & Badia, A. P. (2016). Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626), 471-476.
- Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., & Bengio, Y. (2015). Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning (pp. 2048-2057).
- Reaction post is requried for this class and the following classes (due midnight the night before class)

###  Intuitive physics (part 1: humans)

- Building machines that learn and think like people (Section 4 through 4.1, pg. 9-11)
- Spelke, E. S. (1990). Principles of object perception. Cognitive Science 14(1):29–56.
- Xu, F., & Carey, S. (1996). Infants’ metaphysics: The case of numerical identity. Cognitive psychology, 30(2), 111-153.
- Battaglia, P. W., Hamrick, J. B. & Tenenbaum, J. B. (2013). Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences 110(45):18327–32.

### Intuitive physics (part 2: machines)

- Lerer, A., Gross, S. & Fergus, R. (2016). Learning physical intuition of block towers by example. Presented at the 33rd International Conference on Machine Learning (ICML).
- Battaglia, P., Pascanu, R., Lai, M. & Rezende, D. J. (2016). Interaction networks for learning about objects, relations and physics. Advances in Neural Information Processing Systems.
- Mottaghi, R., Bagherinezhad, H., Rastegari, M., & Farhadi, A. (2016). Newtonian scene understanding: Unfolding the dynamics of objects in static images. Computer Vision and Pattern Recognition (pp. 3521-3529).

###  Intuitive psychology (part 1: humans)

- Building machines that learn and think like people (Section 4.1.2, pg. 11-2)
- Woodward, A. L. (1998). Infants selectively encode the goal object of an actor’s reach. Cognition, 69(1), 1-34.
- Csibra, G., Biro, S., Koos, O. & Gergely, G. (2003). One-year-old infants use teleological representations of actions productively. Cognitive Science 27:111–33
- Baker, C. L., Jara-Ettinger, J., Saxe, R. & Tenenbaum, J. B. (2017). Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. Nature Human Behaviour.

###  Intuitive psychology (part 2: machines)

- Raileanu, R., Denton, E., Szlam, A., and Fergus, R. (2018). Modeling Others using Oneself in Multi-Agent Reinforcement Learning. Proceedings of the 35th International Conference on Machine Learning (ICML).
- Rabinowitz, N. C., Perbet, F., Song, H. F., Eslami, S. M. A., Botvinick, M. (2018). Machine theory of mind. Proceedings of the 35th International Conference on Machine Learning (ICML).

3/21 NO CLASS. Spring Recess
3/28 Compositionality

- Building machines that learn and think like people (Section 4.2-4.2.1, pg. 12-15)
- Marcus, G. (1998) Rethinking eliminative connectionism. Cognitive Psychology 282 (37):243–82.
- Lake, B. M., Linzen, T., and Baroni, M. (2019). Human few-shot learning of compositional instructions. Preprint available on arXiv:1901.04587.
- Reed, S., & De Freitas, N. (2016). Neural programmer-interpreters. International Conference on Learning Representations (ICLR).

###  Causality

- Building machines that learn and think like people (Section 4.2.2, pg. 15-16)
- Murphy, G. L. & Medin, D. L. (1985) The role of theories in conceptual coherence. Psychological Review 92(3):289–316.
- Lake, B. M., Salakhutdinov, R. & Tenenbaum, J. B. (2015) Human-level concept learning through probabilistic program induction. Science 350(6266):1332–38.
- Hewitt, L. B., Nye, M. I., Gane, A., Jaakkola, T., & Tenenbaum, J. B. (2018). The Variational Homoencoder: Learning to learn high capacity generative models from few examples. Uncertainty in Artificial Intelligence (UAI).

###  Learning-to-learn

- Building machines that learn and think like people (Section 4.2.3-4.3, pg. 16-19)
- Smith, L. B., Jones, S. S., Landau, B., Gershkoff-Stowe, L. & Samuelson, L. (2002) Object name learning provides on-the-job training for attention. Psychological Science 13(1):13–19.
- Ritter, S., Barrett, D. G., Santoro, A., & Botvinick, M. M. (2017). Cognitive psychology for deep neural networks: A shape bias case study. International Conference on Machine Learning (ICML).
- Snell, J., Swersky, K., & Zemel, R. (2017). Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems (NIPS).
- Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., … & Botvinick, M. (2016). Learning to reinforcement learn. arXiv preprint arXiv:1611.05763.

###  Critiques of “Building machines that learn and think like people”

- Building machines that learn and think like people (Section 5-end, pg. 19-25)
- Commentaries to read:
    - Botvinick et al., “Building machines that learn and think for themselves”
    - Caglar and Hanson, “Back to the future: The return of cognitive functionalism”
    - Chater and Oaksford, “Theories or fragments?”
    - Clegg and Corriveu, “Children begin with the same start-up software, but their software updates are cultural “
    - Davis and Marcus, “Causal generative models are just a start”
    - Dennet and Lambert, “Thinking like animals or like colleagues?”
    - Hanson, Lampinen, Suriv, McClelland, “Building on prior knowledge without building it in”
    - MacLennan, “Benefits of embodiment”
    - Moerman, “The argument for single-purpose robots”
    - Pierre-Yves Oudeyer, “Autonomous development annd learning in AI and robotics: Scaling up deep learning to human-like learning”
    - Spelke and Blass, “Intelligent machines and human minds”
    - Tessler, Goodman, Frank, “Avoiding frostbite: It helps to learn from others”
- Response, Lake, Ullman, Gershman, Tenenbaum, “Ingredients of intelligence: From classic debates to an engineering roadmap” (pg. 50-59)

###  Language and Culture

- Mikolov, T., Joulin, A. & Baroni, M. (2016) A roadmap towards machine intelligence. arXiv preprint 1511.08130.
- Lupyan, G. & Bergen, B. (2016) How language programs the mind. Topics in Cognitive Science 8(2):408–24.
- Tomasello, M., Kruger, A. C., & Ratner, H. H. (1993). Cultural learning. Behavioral and Brain Sciences, 16(3), 495-511.

###  Emotion and Egocentric learning

- Smith, L. B., & Slone, L. K. (2017). A developmental approach to machine learning?. Frontiers in psychology, 8, 2124.
- Bambach, S., Crandall, D., Smith, L., & Yu, C. (2018). Toddler-Inspired Visual Object Learning. In Advances in Neural Information Processing Systems.
- Ong, D., Soh, H., Zaki, J., & Goodman, N. (2019). Applying Probabilistic Programming to Affective Computing. IEEE Transactions on Affective Computing.



### Introduction; the classical view (Slides)

- Big Book; Chapter 1

###  Prototype and exemplar theories (Slides)

- Big Book; Chapter 2 and Chapter 3
- Rosch, E., & Mervis, C. B. (1975). Family resemblances: Studies in the internal structure of categories. Cognitive Psychology, 7(4), 573-605.
- Medin, D. L., & Schaffer, M. M. (1978). Context theory of classification learning. Psychological Review, 85, 207-238.

###  Concepts as theories and the knowledge view (Slides)

- Big Book; Chapter 4 (pgs. 94-114) and Chapter 6
- Murphy, G. L., & Medin, D. L. (1985). The role of theories in conceptual coherence. Psychological Review, 92, 289-316.
- Barsalou, L. W. (1983). Ad hoc categories. Memory & cognition, 11(3), 211-227.

###  Computational models of category learning (part 1) (Slides)

- Kruschke, J. L. (1992). ALCOVE: An exemplar-based connectionist model of category learning. Psychological Review, 99, 22-44.

### Computational models of category learning (part 2) (Slides)

- Anderson, J. R. (1991). The adaptive nature of human categorization. Psychological Review, 98(3), 409.
- Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (pp. 1097-1105).
- (Optional reference on probability theory) Russel, S. J., and Norvig, P. Artificial Intelligence: A Modern Approach. Chapter 13, Uncertainty.

### (Note special Tuesday time due to Fall recess) Computational models of category learning (part 3) (Slides)

- Xu, F., & Tenenbaum, J. B. (2007). Word learning as Bayesian inference. Psychological Review, 114(2), 245.
- Goodman, N. D., Tenenbaum, J. B., Feldman, J., & Griffiths, T. L. (2008). A rational analysis of rule‐based concept learning. Cognitive Science, 32(1), 108-154.

###  Computational models of category learning (part 4) (Slides)

- Heit, E., & Bott, L. (2000). Knowledge selection in category learning. In Psychology of learning and motivation (Vol. 39, pp. 163-199). Academic Press.
- Rehder, B. (2007). Essentialism as a generative theory of classification. In A. Gopnik, & L. Schultz (Eds.), Causal learning: Psychology, philosophy, and computation (pp. 190-207). Oxford, England: Oxford University Press.

###  Taxonomic organization and the basic level (Slides)

- Big Book; Chapter 7
- Rosch, E., Mervis, C. B., Gray, W. Johnson, D., & Boyes-Braem, P. (1976). Basic objects in natural categories. Cognitive Psychology, 8, 382-439.
- Tanaka, J. W., & Taylor, M. (1991). Object categories and expertise: Is the basic level in the eye of the beholder?. Cognitive Psychology, 23(3), 457-482.

###  Category-based induction (Slides)

- Big Book; Chapter 8
- Osherson, D. N., Smith, E. E., Wilkie, O., Lopez, A., & Shafir, E. (1990). Category-based induction. Psychological Review, 97, 185-200.
- Kemp, C., & Tenenbaum, J. B. (2009). Structured statistical models of inductive reasoning. Psychological Review, 116(1), 20.

###  Concepts in infancy (Slides)

- Big Book; Chapter 9
- Mandler, J. M., & McDonough, L. (1993). Concept formation in infancy. Cognitive Development, 8, 291-318.
- Quinn, P. C. (2004). Development of subordinate-level categorization in 3- to 7-month-old infants. Child Development, 75, 886-899.
###  Conceptual development (Slides)

- Big Book; Chapter 10
- Markman, E. M. (1989). Categorization and naming in children: Problems of induction. Cambridge, MA: MIT Press. (excerpts only)
- Gelman, S. A. (2003). The essential child. Oxford: Oxford University Press. (excerpts only)

### How categories influence perception (Slides)

- Goldstone, R. L., & Hendrickson, A. T. (2010). Categorical perception. Wiley Interdisciplinary Reviews: Cognitive Science, 1(1), 69-78.
- Goldstone, R. L. (1994). Influences of categorization on perceptual discrimination. Journal of Experimental Psychology: General, 123(2), 178.
- Schyns, P. G., & Rodet, L. (1997). Categorization creates functional features. Journal of Experimental Psychology: Learning, Memory, and Cognition, 23(3), 681.

12/2 Conceptual combination and exemplar generation (Slides)

- Big Book; Chapters 12 and 13
- Murphy, G. L. (1988). Comprehending complex concepts. Cognitive Science, 12(4), 529-562.
- Ward, T. B. (1994). Structured imagination: The role of category structure in exemplar generation. Cognitive Psychology, 27(1), 1-40.

